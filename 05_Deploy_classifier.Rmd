---
title: "05_Deploy_classifier"
author: "Victor Yuan"
date: "November 14, 2018"
output:
  html_document:
    keep_md: true
    toc: true
    toc_float: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

This script is for extracting the final model and saving it for publication. To make this tool more 
user-friendly, I create a function that wraps the prediction to output the following variables:

* Probabilities that a sample belongs to a specific class (1 for each ethnicity)
* A class label determined by the highest class-specific probability
* A class label determined after applying a user-supplied threshold function for 'ambiguous' samples
* The highest class-specific probability, used to determine the threshold

# Libraries and data

```{r}
library(dplyr)
library(impute)
library(caret)
library(glmnet)
library(broom)

# Load model and data
GLM_cv <- readRDS('../../Robjects_final/02_GLM_cv_logloss.rds')
betas <- readRDS('../../Robjects_final/01_processed_betas_EPIC.rds') 
dim(betas) #  319233    510
pDat <- readRDS('../../Robjects_final/01_pDat.rds')
dim(pDat) # 510 22

# knn impute
sum(is.na(betas)) # 339
set.seed(1)
betas <- impute.knn(as.matrix(betas), maxp = 15000)$data

#subset out south asians
betas_SA <- betas[,which(pDat$Ethnicity == 'South_Asian')]
pDat_SA <- pDat[which(pDat$Ethnicity == 'South_Asian'),]
dim(betas_SA);dim(pDat_SA) # 7 samples

glm_fit <- GLM_cv$finalModel
features <- glm_fit$beta$African@Dimnames[[1]]
```

Here I manually create the outputs mentioned above. Later I implement a function that automatically
does this.

# Infer ethnicity 

```{r}
# obtain predictions
preds <- as.data.frame(predict(glm_fit, t(betas_SA), s = glm_fit$lambdaOpt, type = 'class'))
probs <- as.data.frame(predict(glm_fit, t(betas_SA), s = glm_fit$lambdaOpt, type = 'response'))

#combine
pred_prob <- cbind(preds, probs)
colnames(pred_prob) <- c('Predicted_ethnicity_nothresh', paste0('Prob_', glm_fit$classnames))

# calculate highest probability
pred_prob$Highest_Prob <- apply(pred_prob[,2:4], 1, max)

# call ambiguous if below threshold
pred_prob$Predicted_ethnicity <- ifelse(pred_prob$Highest_Prob < 0.75, 'Ambiguous', 
                                        as.character(pred_prob$Predicted_ethnicity_nothresh))
pred_prob$Sample_ID <- rownames(pred_prob)
pred_prob <- pred_prob[,c(7,1,6, 2:5)]
```

# Function: Attempt 1

Now I wrap the above code into a function:

```{r}
pl_infer_ethnicity <- function(betas, threshold = 0.75){ # betas need to be in the form of samples in columns
  if(!all(features %in% rownames(betas))) {
    stop('Rownames of betas df must include all 319233 features used to fit classifier.')
  }
  
  betas <- t(betas[features,])
  
  # obtain predictions
  preds <- as.data.frame(predict(glm_fit, betas, s = glm_fit$lambdaOpt, type = 'class'))
  probs <- as.data.frame(predict(glm_fit, betas, s = glm_fit$lambdaOpt, type = 'response'))
  p <- cbind(preds, probs)
  colnames(p) <- c('Predicted_ethnicity_nothresh', paste0('Prob_', glm_fit$classnames))

  p$Highest_Prob <- apply(p[,2:4], 1, max)
  
  p$Predicted_ethnicity <- ifelse(p$Highest_Prob < threshold, 'Ambiguous', 
                                        as.character(p$Predicted_ethnicity_nothresh))
  p$Sample_ID <- rownames(p)
  p <- p[,c(7,1,6, 2:5)]
  
  return(p)
}

pl_infer_ethnicity(betas_SA, threshold = 0.75)
```

Unfortunately the above code requires that new samples have all the features used for training 
(n=319233), when only 1860 are required for the final prediction. Below I extract the coefficients
from the final model and see if I can create the same predictions using a cross product with the 
sample vector.

# Function: Attempt 2

```{r eval = F}
# extract coefficients
coef <- coef(glm_fit, glm_fit$lambdaOpt)

# bind feature coefficients for each cpg and intercept
out <- do.call("cbind", lapply(coef, function(x) x[,1])) 
out <- as.data.frame(out) %>% mutate(feature = rownames(out)) %>% as_tibble()
out$feature[1] <- 'Intercept'

# should be 1862 + 1 intercept
out <- out %>% filter(abs(African) > 0 | abs(Asian) > 0 | abs(Caucasian) > 0)
out %>% arrange(desc(Asian), desc(African), desc(Caucasian))

#filter data to features
newDat <- betas_SA[out$feature[2:nrow(out)],2]

#cross product, adding 1 for intercept term
af <- out$African %*% c(1, newDat)
as <- out$Asian %*% c(1, newDat)
ca <- out$Caucasian %*% c(1, newDat)

af/sum(af,as,ca)
as/sum(af,as,ca)
ca/sum(af,as,ca)
```

After taking out the coefficients and trying the cross product, I get values that I can't make sense
of. I think I need to implement a loglink function on this output, but I'm not sure how to do this.

Instead, I create a workaround, where given a new samples with the final 1860 features, I add on
'fake' data of the remaining 319233-1860 features so that the predict() function accepts it.

# Function: Attempt 3

```{r}
# create 'new' data of only the necessary features
pl_features <- predictors(GLM_cv)
newDat <- betas_SA[pl_features,]
dim(newDat) #1860 features, 7 samples

pl_infer_ethnicity <- function(betas, threshold = 0.75) {
  
  # find all final predictors in new data
  present_features <- intersect(rownames(betas), pl_features)
  print(paste(length(present_features), 'of 1860 predictors present.'))
  
  dat <- betas[present_features,]
  dim(dat)
  
  # find missing non-predictors used for training
  train_features <- glm_fit$beta$African@Dimnames[[1]]
  missing_features <- setdiff(train_features, present_features)
  
  # Create a matrix of zeros for these missing features
  zeros <- data.frame(
    matrix(data = 0, nrow = length(missing_features), ncol = ncol(betas), 
           dimnames = list(missing_features, colnames(betas))))
  
  # add back into the data
  dat_in <- t(rbind(dat, zeros)[train_features,])
  
  # run prediction
  preds <- as.data.frame(predict(glm_fit, dat_in, s = glm_fit$lambdaOpt, type = 'class'))
  probs <- as.data.frame(predict(glm_fit, dat_in, s = glm_fit$lambdaOpt, type = 'response'))
  p <- cbind(preds, probs)
  colnames(p) <- c('Predicted_ethnicity_nothresh', paste0('Prob_', glm_fit$classnames))
  p$Highest_Prob <- apply(p[,2:4], 1, max)
  p$Predicted_ethnicity <- ifelse(p$Highest_Prob < threshold, 'Ambiguous', 
                                        as.character(p$Predicted_ethnicity_nothresh))
  p$Sample_ID <- rownames(p)
  p <- p[,c(7,1,6, 2:5)]
  
  return(p)
}

#now compare:
pl_infer_ethnicity(newDat)
pred_prob
```

yay

# Function: Attempt 4

Because the above workaround requires publishing a 1.1 Gb model fit object, I try again to find a 
formula for the manual cross-product method.

```{r}
#args
object <-glm_fit
s <- glm_fit$lambdaOpt
type <- "response"
exact <- F
newx <- t(newDat)

a0 <- object$a0 # dunno what this is
rownames(a0) <- rep("(Intercept)", nrow(a0))
nbeta <- object$beta
klam <- dim(a0) # number of lambdas tuned against
nclass <- klam[[1]] # # of classes
nlambda <- length(s) # # of supplied lambda

# if !is.null(s)
lambda <- object$lambda
lamlist <- lambda.interp(lambda, s)
for (i in seq(nclass)) {
  kbeta <- methods::rbind2(a0[i, , drop = FALSE], nbeta[[i]])#was rbind2
  vnames <- dimnames(kbeta)[[1]]
  dimnames(kbeta) <- list(NULL, NULL)
  kbeta <- kbeta[, lamlist$left, drop = FALSE] %*% Diagonal(x=lamlist$frac) +
    kbeta[, lamlist$right, drop = FALSE] %*% Diagonal(x=1 - lamlist$frac)
  dimnames(kbeta) <- list(vnames, paste(seq(along = s)))
  nbeta[[i]] <- kbeta
}

c <- lapply(nbeta, function(x) nonzeroCoef(x[-1, ,drop = FALSE], bystep = TRUE))

dd <- dim(newx)
if (inherits(newx, "sparseMatrix"))
  newx <- as(newx, "dgCMatrix")
npred <- dd[[1]]
dn <- list(names(nbeta), dimnames(nbeta[[1]])[[2]], dimnames(newx)[[1]])
dp <- array(0, c(nclass, nlambda, npred), dimnames = dn)
for (i in seq(nclass)) {
  fitk <- cbind2(1, newx) %*% matrix(nbeta[[i]][c("(Intercept)", colnames(newx)),])
  dp[i, , ] = dp[i, , ] + t(as.matrix(fitk))
}

  switch(type, response = {
    pp <- exp(dp)
    psum <- apply(pp, c(2, 3), sum)
    aperm(pp/rep(psum, rep(nclass, nlambda * npred)), c(3, 
        1, 2))
  }, link <- aperm(dp, c(3, 1, 2)), class = {
    dpp <- aperm(dp, c(3, 1, 2))
    apply(dpp, 3, glmnet_softmax)
  })

predict(glm_fit, t(betas_SA), s = glm_fit$lambdaOpt, type = 'class')
predict(glm_fit, t(betas_SA), s = glm_fit$lambdaOpt, type = 'response')

```

This works too!

# Save data

saveRDS(glm_fit, '../../Robjects_final/05_glm_fit.rds')

For south asian samples amy is using:

```{r}
# add predictions to pDat
pDat_SA <- pDat_SA %>% left_join(pred_prob, by = c('sampleNames' = 'Sample_ID'))

# combine with other samples pDat
pDat_final <- readRDS('../../Robjects_final/03_final_pData.rds')
pDat_save <- pDat_final %>% 
  bind_rows(pDat_SA %>% select(-matAge, -CH, -GA_2) %>% mutate(GA = as.numeric(GA)) %>%
              rename(Self_reported_ethnicity = Ethnicity, Cohort_owner = Dataset))
dim(pDat_save) # 506 33

# recall ambiguous samples based on 0.75 threshold
pDat_save$Predicted_ethnicity <- ifelse(pDat_save$Highest_Prob < 0.75, 'Ambiguous', 
                                        as.character(pDat_save$Predicted_ethnicity))
table(pDat_save$Predicted_ethnicity)
saveRDS(pDat_save, '../../Robjects_final/05_pDat_506samps.rds')

ggplot(pDat_save, aes(x = Prob_Caucasian, y = Prob_African, col = Predicted_ethnicity)) +
  geom_point()

ggplot(pDat_save, aes(x = Prob_Caucasian, y = Prob_African, col = Self_reported_ethnicity)) +
  geom_point()
```